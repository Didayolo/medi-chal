{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing data distributions\n",
    "\n",
    "** We want to compare the different representations and meta-features of two distributions to characterize their similarities and differences (e.g. original data VS generated data). **\n",
    "\n",
    "- Data format : autoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'iris': ('sample_data/iris', 'iris'),\n",
    "            'iris_1': ('sample_data/iris_1', 'iris'),\n",
    "            'iris_2': ('sample_data/iris_2', 'iris'),\n",
    "            'mimic': ('sample_data/mimic_data', 'mimic'),\n",
    "            'mushrooms': ('sample_data/mushrooms', 'mushrooms'),\n",
    "            'chems': ('sample_data/chems', 'chems'),\n",
    "            'credit': ('sample_data/credit_data', 'credit'),\n",
    "            'squares': ('sample_data/squares', 'squares'),\n",
    "            'squares_2': ('sample_data/squares_2', 'squares')}\n",
    "\n",
    "# First dataset\n",
    "input_dir1, basename1 = datasets['squares']\n",
    "\n",
    "# Second dataset\n",
    "input_dir2, basename2 = datasets['squares_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "- ** Overall meta-features ** (descriptors): we compute simple distances between the descriptors of each dataset.\n",
    "- ** Individual features/variables ** (column comparison):\n",
    "\n",
    "    - Numerical:\n",
    "        - Kolmogorov-Smirnov test\n",
    "        \n",
    "    - Categorical:\n",
    "        - TODO\n",
    "    \n",
    "    - Other:\n",
    "        - Mutual information score: This is equal to the Kullback-Leibler divergence of the joint distribution with the product distribution of the marginals.\n",
    "        - Kullback-Leibler divergence\n",
    "\n",
    "- ** Discriminant ** (row comparison): we label the data with 0 or 1 according to their original dataset and then train a binary classifier on it. This is the method used to train GANs. More sophisticated the classifier which succeeds in separating the data is, more similar they are. If the classifier can't separate the data, maybe they are to similar, maybe the classifier isn't good enough. \n",
    "- ** Landmark: ** performance in prediction of the target among various models and metrics.\n",
    "- ** Change of representations: ** we train an auto-encoder on dataset A and benchmark it on dataset B (and reciprocally). The intuition behind this is that similar data will be compressible in the same latent space. This principle could be applied to other changes of representation.\n",
    "- ** Causal inference: ** comparison of causal inference results. Do we notice the same causal links between the variables?\n",
    "\n",
    "Draft:\n",
    "- Jensen Shannon divergence/distance\n",
    "- Wasserstein distance (minimum cost of turning one \"pile of dirt\" into the other)\n",
    "- Chi square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# AutoML and Comparator\n",
    "problem_dir = 'data_manager/'  \n",
    "from sys import path\n",
    "path.append(problem_dir)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from auto_ml import AutoML\n",
    "from comparator import Comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No info file file found.\n",
      "No info file file found.\n"
     ]
    }
   ],
   "source": [
    "comparator = Comparator(AutoML(input_dir1, basename1), AutoML(input_dir2, basename2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.0029773423133451077\n",
      "Skewness min: 0.05149283001005989\n",
      "Skewness max: 1.4798295459483288\n",
      "Skewness mean: 0.07777590895847464\n"
     ]
    }
   ],
   "source": [
    "#comparator.compare_descriptors(norm='euclidean')\n",
    "comparator.show_descriptors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual features comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.0.1</th>\n",
       "      <th>1.0.2</th>\n",
       "      <th>1.0.3</th>\n",
       "      <th>1.0.4</th>\n",
       "      <th>1.0.5</th>\n",
       "      <th>0.0.2</th>\n",
       "      <th>0.0.3</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0.54</th>\n",
       "      <th>0.0.55</th>\n",
       "      <th>0.0.56</th>\n",
       "      <th>0.0.57</th>\n",
       "      <th>0.0.58</th>\n",
       "      <th>0.0.59</th>\n",
       "      <th>0.0.60</th>\n",
       "      <th>0.0.61</th>\n",
       "      <th>0.0.62</th>\n",
       "      <th>0.0.63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kullback-Leibler divergence</th>\n",
       "      <td>(0.00014, 0.00013)</td>\n",
       "      <td>(8e-05, 8e-05)</td>\n",
       "      <td>(0.0002, 0.00019)</td>\n",
       "      <td>(0.00025, 0.00026)</td>\n",
       "      <td>(0.00028, 0.00029)</td>\n",
       "      <td>(0.0006, 0.00062)</td>\n",
       "      <td>(0.00067, 0.00071)</td>\n",
       "      <td>(0.00045, 0.00049)</td>\n",
       "      <td>(0.00055, 0.00059)</td>\n",
       "      <td>(0.00066, 0.00076)</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.00081, 0.00074)</td>\n",
       "      <td>(0.00087, 0.00081)</td>\n",
       "      <td>(0.00067, 0.00063)</td>\n",
       "      <td>(0.00021, 0.00021)</td>\n",
       "      <td>(0.0001, 0.0001)</td>\n",
       "      <td>(5e-05, 5e-05)</td>\n",
       "      <td>(0.00018, 0.00018)</td>\n",
       "      <td>(0.00011, 0.00011)</td>\n",
       "      <td>(0.00028, 0.00031)</td>\n",
       "      <td>(5e-05, 5e-05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mutual information</th>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>1.09861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kolmogorov-Smirnov</th>\n",
       "      <td>(0.97214, 0.0)</td>\n",
       "      <td>(0.94642, 0.0)</td>\n",
       "      <td>(0.91985, 0.0)</td>\n",
       "      <td>(0.89841, 0.0)</td>\n",
       "      <td>(0.87998, 0.0)</td>\n",
       "      <td>(0.8787, 0.0)</td>\n",
       "      <td>(0.89882, 0.0)</td>\n",
       "      <td>(0.91849, 0.0)</td>\n",
       "      <td>(0.94549, 0.0)</td>\n",
       "      <td>(0.97233, 0.0)</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.97185, 0.0)</td>\n",
       "      <td>(0.94585, 0.0)</td>\n",
       "      <td>(0.91927, 0.0)</td>\n",
       "      <td>(0.89727, 0.0)</td>\n",
       "      <td>(0.87798, 0.0)</td>\n",
       "      <td>(0.87741, 0.0)</td>\n",
       "      <td>(0.89413, 0.0)</td>\n",
       "      <td>(0.91656, 0.0)</td>\n",
       "      <td>(0.94432, 0.0)</td>\n",
       "      <td>(0.971, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            0.0           0.0.1  \\\n",
       "Kullback-Leibler divergence  (0.00014, 0.00013)  (8e-05, 8e-05)   \n",
       "Mutual information                      1.09861         1.09861   \n",
       "Kolmogorov-Smirnov               (0.97214, 0.0)  (0.94642, 0.0)   \n",
       "\n",
       "                                           1.0               1.0.1  \\\n",
       "Kullback-Leibler divergence  (0.0002, 0.00019)  (0.00025, 0.00026)   \n",
       "Mutual information                     1.09861             1.09861   \n",
       "Kolmogorov-Smirnov              (0.91985, 0.0)      (0.89841, 0.0)   \n",
       "\n",
       "                                          1.0.2              1.0.3  \\\n",
       "Kullback-Leibler divergence  (0.00028, 0.00029)  (0.0006, 0.00062)   \n",
       "Mutual information                      1.09861            1.09861   \n",
       "Kolmogorov-Smirnov               (0.87998, 0.0)      (0.8787, 0.0)   \n",
       "\n",
       "                                          1.0.4               1.0.5  \\\n",
       "Kullback-Leibler divergence  (0.00067, 0.00071)  (0.00045, 0.00049)   \n",
       "Mutual information                      1.09861             1.09861   \n",
       "Kolmogorov-Smirnov               (0.89882, 0.0)      (0.91849, 0.0)   \n",
       "\n",
       "                                          0.0.2               0.0.3  \\\n",
       "Kullback-Leibler divergence  (0.00055, 0.00059)  (0.00066, 0.00076)   \n",
       "Mutual information                      1.09861             1.09861   \n",
       "Kolmogorov-Smirnov               (0.94549, 0.0)      (0.97233, 0.0)   \n",
       "\n",
       "                                  ...                    0.0.54  \\\n",
       "Kullback-Leibler divergence       ...        (0.00081, 0.00074)   \n",
       "Mutual information                ...                   1.09861   \n",
       "Kolmogorov-Smirnov                ...            (0.97185, 0.0)   \n",
       "\n",
       "                                         0.0.55              0.0.56  \\\n",
       "Kullback-Leibler divergence  (0.00087, 0.00081)  (0.00067, 0.00063)   \n",
       "Mutual information                      1.09861             1.09861   \n",
       "Kolmogorov-Smirnov               (0.94585, 0.0)      (0.91927, 0.0)   \n",
       "\n",
       "                                         0.0.57            0.0.58  \\\n",
       "Kullback-Leibler divergence  (0.00021, 0.00021)  (0.0001, 0.0001)   \n",
       "Mutual information                      1.09861           1.09861   \n",
       "Kolmogorov-Smirnov               (0.89727, 0.0)    (0.87798, 0.0)   \n",
       "\n",
       "                                     0.0.59              0.0.60  \\\n",
       "Kullback-Leibler divergence  (5e-05, 5e-05)  (0.00018, 0.00018)   \n",
       "Mutual information                  1.09861             1.09861   \n",
       "Kolmogorov-Smirnov           (0.87741, 0.0)      (0.89413, 0.0)   \n",
       "\n",
       "                                         0.0.61              0.0.62  \\\n",
       "Kullback-Leibler divergence  (0.00011, 0.00011)  (0.00028, 0.00031)   \n",
       "Mutual information                      1.09861             1.09861   \n",
       "Kolmogorov-Smirnov               (0.91656, 0.0)      (0.94432, 0.0)   \n",
       "\n",
       "                                     0.0.63  \n",
       "Kullback-Leibler divergence  (5e-05, 5e-05)  \n",
       "Mutual information                  1.09861  \n",
       "Kolmogorov-Smirnov             (0.971, 0.0)  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparator.show_comparison_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Score: 0.5384615384615384\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "\n",
      "Score: 0.08692307692307692\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comparator.show_classifier_score()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "comparator.show_classifier_score(clf=RandomForestClassifier(n_estimators=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if same number of samples !\n",
    "#comparator.dcov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if same number of samples !\n",
    "# Norm = 'l0',\n",
    "#        'manhattan' or 'l1', \n",
    "#        'euclidean' or 'l2',\n",
    "#        'minimum',\n",
    "#        'maximum',\n",
    "#comparator.datasets_distance(axis=0, norm='manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
